package com.rapidcrud.generator.kafka;

import lombok.extern.slf4j.Slf4j;
import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.common.KafkaFuture;
import org.apache.kafka.common.serialization.StringDeserializer;
import org.springframework.context.annotation.Configuration;
import org.springframework.kafka.annotation.EnableKafka;
import org.springframework.kafka.config.ConcurrentKafkaListenerContainerFactory;
import org.springframework.context.annotation.Bean;
import org.springframework.kafka.core.ConsumerFactory;
import org.springframework.kafka.core.DefaultKafkaConsumerFactory;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.kafka.listener.ConsumerRecordRecoverer;
import org.springframework.kafka.listener.ContainerProperties;
import org.springframework.kafka.listener.DefaultErrorHandler;
import org.springframework.kafka.support.serializer.JsonDeserializer;
import org.springframework.util.backoff.BackOff;
import org.springframework.util.backoff.BackOffExecution;
import java.util.function.BiConsumer;

import java.util.HashMap;
import java.util.Map;
import java.util.Random;

@Slf4j
@EnableKafka
@Configuration
public class KafkaConsumerConfig {

    @Bean
    public DefaultErrorHandler errorHandler(KafkaTemplate<String, AuditLogEvent> kafkaTemplate) {
        // ğŸŒ€ Retry up to 3 times with jitter (2s Â± 1s)
        BackOff backOff = new BackOff() {
            private final Random random = new Random();
            private final int maxRetries = 3;
            @Override
            public BackOffExecution start() {
                return new BackOffExecution() {
                    int attempt = 0;
                    @Override
                    public long nextBackOff() {
                        if (attempt++ >= maxRetries) return BackOffExecution.STOP;
                        return 2000 + random.nextInt(1000); // jitter
                    }
                };
            }
        };

        ConsumerRecordRecoverer recoverer = (record, ex) -> {
            log.error("âŒ Reached max retry. Sending to DLQ. Record: {}", record.value());
            kafkaTemplate.send("audit-log-dlt", record.key().toString(), (AuditLogEvent) record.value());
        };

        return new DefaultErrorHandler(recoverer, backOff);
    }


    @Bean
    public ConsumerFactory<String, AuditLogEvent> consumerFactory() {
        StringDeserializer keyDeserializer = new StringDeserializer();

        JsonDeserializer<AuditLogEvent> valueDeserializer = new JsonDeserializer<>(AuditLogEvent.class);
        valueDeserializer.addTrustedPackages("*");

        Map<String, Object> props = new HashMap<>();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.put(ConsumerConfig.GROUP_ID_CONFIG, "audit-consumer-group");

        return new DefaultKafkaConsumerFactory<>(
                props,
                keyDeserializer,  // âœ… æ˜¾å¼ä¼ å…¥ key deserializer
                valueDeserializer               // âœ… æ˜¾å¼ä¼ å…¥ value deserializer å®ä¾‹
        );
    }

    @Bean
    public ConcurrentKafkaListenerContainerFactory<String, AuditLogEvent> kafkaListenerContainerFactory(KafkaTemplate<String, AuditLogEvent> kafkaTemplate) {
        ConcurrentKafkaListenerContainerFactory<String, AuditLogEvent> factory =
                new ConcurrentKafkaListenerContainerFactory<>();
        factory.setConsumerFactory(consumerFactory());

        // âœ… è®¾ç½®æ‰‹åŠ¨ ACK æ¨¡å¼
        factory.getContainerProperties().setAckMode(ContainerProperties.AckMode.MANUAL_IMMEDIATE);

        // âœ… è®¾ç½®æ¶ˆè´¹çº¿ç¨‹å¹¶å‘æ•°ï¼ˆå¦‚æœéœ€è¦ï¼‰
        factory.setConcurrency(3);

        // â¬‡ï¸ åŠ å…¥ error handler
        factory.setCommonErrorHandler(errorHandler(kafkaTemplate));

        return factory;
    }
}
